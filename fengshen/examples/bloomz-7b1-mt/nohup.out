++ date +%s
+ run_ts=1671234458
++ run_ts
./fintune_bloomz_mt_lowsub_highinter.sh: line 16: run_ts: command not found
+ echo 'RUN TS: '
RUN TS: 
++ date
+ echo 'START TIME: Sat Dec 17 07:47:38 CST 2022'
START TIME: Sat Dec 17 07:47:38 CST 2022
+ MICRO_BATCH_SIZE=1
+ ROOT_DIR=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid
+ '[' '!' -d /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid ']'
+ echo /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid 'exist!!!!!!!!!!!!!!!'
/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid exist!!!!!!!!!!!!!!!
+ ZERO_STAGE=2
+ config_json=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/training_config.json
+ export MASTER_PORT=39124
+ MASTER_PORT=39124
+ cat
+ export PL_DEEPSPEED_CONFIG_PATH=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/training_config.json
+ PL_DEEPSPEED_CONFIG_PATH=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/training_config.json
+ checkpoint_path=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458
+ export TORCH_EXTENSIONS_DIR=/home/ubuntu/cloudfs/torch_extendsions
+ TORCH_EXTENSIONS_DIR=/home/ubuntu/cloudfs/torch_extendsions
+ TRAINER_ARGS='
    --max_epochs 50     --gpus 8     --num_nodes 1     --strategy deepspeed_stage_2     --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid     --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458     --save_top_k 2     --monitor train_loss     --mode min     --save_last     --check_val_every_n_epoch 1 '
+ DATA_DIR=/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/
+ train_data_file=/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv
+ val_data_file=/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv
+ DATA_ARGS='
    --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/     --train_batchsize 1     --valid_batchsize 1     --train_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv     --valid_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --test_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --max_seq_length 700 '
+ PRETRAINED_MODEL_PATH=/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt
+ MODEL_ARGS='
    --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt     --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json     --learning_rate 4e-5     --weight_decay 0.1     --warmup 0.01     --run_ts 1671234458 '
+ SCRIPTS_PATH=/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py
+ export 'CMD=     /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py     
    --max_epochs 50     --gpus 8     --num_nodes 1     --strategy deepspeed_stage_2     --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid     --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458     --save_top_k 2     --monitor train_loss     --mode min     --save_last     --check_val_every_n_epoch 1      
    --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt     --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json     --learning_rate 4e-5     --weight_decay 0.1     --warmup 0.01     --run_ts 1671234458      
    --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/     --train_batchsize 1     --valid_batchsize 1     --train_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv     --valid_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --test_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --max_seq_length 700      '
+ CMD='     /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py     
    --max_epochs 50     --gpus 8     --num_nodes 1     --strategy deepspeed_stage_2     --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid     --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458     --save_top_k 2     --monitor train_loss     --mode min     --save_last     --check_val_every_n_epoch 1      
    --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt     --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json     --learning_rate 4e-5     --weight_decay 0.1     --warmup 0.01     --run_ts 1671234458      
    --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/     --train_batchsize 1     --valid_batchsize 1     --train_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv     --valid_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --test_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --max_seq_length 700      '
+ echo /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py --max_epochs 50 --gpus 8 --num_nodes 1 --strategy deepspeed_stage_2 --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458 --save_top_k 2 --monitor train_loss --mode min --save_last --check_val_every_n_epoch 1 --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json --learning_rate 4e-5 --weight_decay 0.1 --warmup 0.01 --run_ts 1671234458 --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/ --train_batchsize 1 --valid_batchsize 1 --train_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv --valid_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --test_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --max_seq_length 700
/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py --max_epochs 50 --gpus 8 --num_nodes 1 --strategy deepspeed_stage_2 --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458 --save_top_k 2 --monitor train_loss --mode min --save_last --check_val_every_n_epoch 1 --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json --learning_rate 4e-5 --weight_decay 0.1 --warmup 0.01 --run_ts 1671234458 --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/ --train_batchsize 1 --valid_batchsize 1 --train_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv --valid_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --test_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --max_seq_length 700
+ python /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py --max_epochs 50 --gpus 8 --num_nodes 1 --strategy deepspeed_stage_2 --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458 --save_top_k 2 --monitor train_loss --mode min --save_last --check_val_every_n_epoch 1 --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json --learning_rate 4e-5 --weight_decay 0.1 --warmup 0.01 --run_ts 1671234458 --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/ --train_batchsize 1 --valid_batchsize 1 --train_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv --valid_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --test_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --max_seq_length 700
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=8)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=8)` instead.
  rank_zero_deprecation(
Loading DeepSpeed config from set PL_DEEPSPEED_CONFIG_PATH environment variable
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8
initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8
initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671234458', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671234458', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8
initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2182: LightningDeprecationWarning: `Trainer.gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` or `Trainer.device_ids` to get device information instead.
  rank_zero_deprecation(
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
You have specified an optimizer and/or scheduler within the DeepSpeed config. It is recommended to define it in `LightningModule.configure_optimizers`.

  | Name  | Type             | Params
-------------------------------------------
0 | model | BloomForCausalLM | 7.1 B 
-------------------------------------------
7.1 B     Trainable params
0         Non-trainable params
7.1 B     Total params
28,276.064Total estimated model params size (MB)
Rank: 0 partition count [8] and sizes[(883627008, False)] 
Sanity Checking: 0it [00:00, ?it/s]/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 164 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Rank: 1 partition count [8] and sizes[(883627008, False)] 
Rank: 2 partition count [8] and sizes[(883627008, False)] 
Rank: 6 partition count [8] and sizes[(883627008, False)] 
Rank: 3 partition count [8] and sizes[(883627008, False)] 
Rank: 5 partition count [8] and sizes[(883627008, False)] 
Rank: 4 partition count [8] and sizes[(883627008, False)] 
Rank: 7 partition count [8] and sizes[(883627008, False)] 
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  1.20s/it]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.63it/s]validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.1930396556854248, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]暗黑系'), (-0.19889667630195618, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗色美甲]')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]'), (-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.1274288147687912, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊羔肉]'), (-0.13461348414421082, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊腿肉：]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.1624516099691391, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真無線藍牙耳機：]'), (-0.19972708821296692, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真·无线蓝牙耳机]')]
                                                                           /home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 164 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/10549 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/10549 [00:00<?, ?it/s] validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.17055103182792664, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗黑色系美甲]'), (-0.22488616406917572, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗夜女王]')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.12402424216270447, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[智能扫地机器人]'), (-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.1494964063167572, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊：]'), (-0.1520892232656479, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原鲜羊肉：]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.18360446393489838, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真無線藍牙耳機]'), (-0.28871950507164, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真耳机]')]
validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.15172182023525238, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]暗黑美甲'), (-0.13014498353004456, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲教程：]')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]'), (-0.1477939635515213, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机：]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.15023694932460785, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊腿肉]'), (-0.161230206489563, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.2661653161048889, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真蓝牙耳机：小红书]'), (-0.19972708821296692, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真·无线蓝牙耳机]')]
validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.16700030863285065, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗夜魅惑美甲]'), (-0.17427784204483032, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]小红书爆款标题')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]'), (-0.21379344165325165, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]小红书扫地机器人测评')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.13461348414421082, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊腿肉：]'), (-0.1494964063167572, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊：]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.1624516099691391, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真無線藍牙耳機：]'), (-0.2184860110282898, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真蓝牙耳机]')]
validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.1476762294769287, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]'), (-0.17910818755626678, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗夜系美甲]')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.15296441316604614, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[关于扫地机器人的内容]'), (-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.147514209151268, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊腿：]'), (-0.161230206489563, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.24596327543258667, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真蓝牙耳机：大牌耳机和杂牌耳机哪个好]'), (-0.271414577960968, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真蓝牙耳机：）]')]
validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.15172182023525238, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]暗黑美甲'), (-0.16700030863285065, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗夜魅惑美甲]')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]'), (-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.1762756109237671, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原大肉：]'), (-0.11621568351984024, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊羔肉：]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.1624516099691391, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真無線藍牙耳機：]'), (-0.1624516099691391, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真無線藍牙耳機：]')]
validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.17055103182792664, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[暗黑色系美甲]'), (-0.1476762294769287, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]'), (-0.17488206923007965, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.1274288147687912, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊羔肉]'), (-0.147514209151268, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊腿：]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.1624516099691391, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真無線藍牙耳機：]'), (-0.19972708821296692, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真·无线蓝牙耳机]')]
validation_epoch_end...
--0/4--- input text: 暗黑系美甲：
validation_samples:
predictions: [(-0.20291553437709808, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲]少女心爆棚的淡粉色美甲'), (-0.16803666949272156, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[暗黑系美甲：]\n小红书标题：[美甲] 暗黑美甲')]
--1/4--- input text: 扫地机器人：
validation_samples:
predictions: [(-0.18667040765285492, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[扫拖一体机器人]'), (-0.2621272802352905, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[扫地机器人：]\n小红书标题：[小红书：扫地机器人]\n\n[产品：扫地机]')]
--2/4--- input text: 草原羊肉：
validation_samples:
predictions: [(-0.1520892232656479, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原鲜羊肉：]'), (-0.11621568351984024, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[草原羊肉：]\n小红书标题：[草原羊羔肉：]')]
--3/4--- input text: 真无线蓝牙耳机：
validation_samples:
predictions: [(-0.1622788906097412, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[蓝牙耳机真无线]'), (-0.17669779062271118, '根据指定内容，撰写爆款小红书笔记标题：\n：需要起标题的内容：[真无线蓝牙耳机：]\n小红书标题：[真蓝牙耳机]\n\n小红书爆款标题：[蓝牙耳机]')]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Epoch 0:   0%|          | 1/10549 [00:02<7:34:16,  2.58s/it]Epoch 0:   0%|          | 1/10549 [00:02<7:34:25,  2.58s/it, loss=3.37, v_num=1]Epoch 0:   0%|          | 2/10549 [00:03<4:39:51,  1.59s/it, loss=3.37, v_num=1]Epoch 0:   0%|          | 2/10549 [00:03<4:39:53,  1.59s/it, loss=3.65, v_num=1]Epoch 0:   0%|          | 3/10549 [00:03<3:51:50,  1.32s/it, loss=3.65, v_num=1]Epoch 0:   0%|          | 3/10549 [00:03<3:51:52,  1.32s/it, loss=3.58, v_num=1]Epoch 0:   0%|          | 4/10549 [00:04<3:29:53,  1.19s/it, loss=3.58, v_num=1]Epoch 0:   0%|          | 4/10549 [00:04<3:29:54,  1.19s/it, loss=3.52, v_num=1]Epoch 0:   0%|          | 5/10549 [00:05<3:12:27,  1.10s/it, loss=3.52, v_num=1]Epoch 0:   0%|          | 5/10549 [00:05<3:12:28,  1.10s/it, loss=3.53, v_num=1]Epoch 0:   0%|          | 6/10549 [00:06<3:05:57,  1.06s/it, loss=3.53, v_num=1]Epoch 0:   0%|          | 6/10549 [00:06<3:05:58,  1.06s/it, loss=3.57, v_num=1]Epoch 0:   0%|          | 7/10549 [00:07<2:56:48,  1.01s/it, loss=3.57, v_num=1]Epoch 0:   0%|          | 7/10549 [00:07<2:56:49,  1.01s/it, loss=3.64, v_num=1]Epoch 0:   0%|          | 8/10549 [00:07<2:53:41,  1.01it/s, loss=3.64, v_num=1]Epoch 0:   0%|          | 8/10549 [00:07<2:53:42,  1.01it/s, loss=3.69, v_num=1]Epoch 0:   0%|          | 9/10549 [00:08<2:48:16,  1.04it/s, loss=3.69, v_num=1]Epoch 0:   0%|          | 9/10549 [00:08<2:48:17,  1.04it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 10/10549 [00:09<2:46:45,  1.05it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 10/10549 [00:09<2:46:46,  1.05it/s, loss=3.78, v_num=1]Epoch 0:   0%|          | 11/10549 [00:10<2:42:39,  1.08it/s, loss=3.78, v_num=1]Epoch 0:   0%|          | 11/10549 [00:10<2:42:40,  1.08it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 12/10549 [00:10<2:40:32,  1.09it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 12/10549 [00:10<2:40:32,  1.09it/s, loss=3.8, v_num=1] Epoch 0:   0%|          | 13/10549 [00:11<2:37:39,  1.11it/s, loss=3.8, v_num=1]Epoch 0:   0%|          | 13/10549 [00:11<2:37:40,  1.11it/s, loss=3.77, v_num=1]Epoch 0:   0%|          | 14/10549 [00:12<2:37:20,  1.12it/s, loss=3.77, v_num=1]Epoch 0:   0%|          | 14/10549 [00:12<2:37:21,  1.12it/s, loss=3.8, v_num=1] Epoch 0:   0%|          | 15/10549 [00:13<2:34:57,  1.13it/s, loss=3.8, v_num=1]Epoch 0:   0%|          | 15/10549 [00:13<2:34:58,  1.13it/s, loss=3.77, v_num=1]Epoch 0:   0%|          | 16/10549 [00:14<2:33:50,  1.14it/s, loss=3.77, v_num=1]Epoch 0:   0%|          | 16/10549 [00:14<2:33:50,  1.14it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 17/10549 [00:14<2:31:59,  1.15it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 17/10549 [00:14<2:31:59,  1.15it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 18/10549 [00:15<2:32:00,  1.15it/s, loss=3.73, v_num=1]Epoch 0:   0%|          | 18/10549 [00:15<2:32:01,  1.15it/s, loss=3.81, v_num=1]Epoch 0:   0%|          | 19/10549 [00:16<2:30:38,  1.16it/s, loss=3.81, v_num=1]Epoch 0:   0%|          | 19/10549 [00:16<2:30:39,  1.16it/s, loss=3.81, v_num=1]Epoch 0:   0%|          | 20/10549 [00:17<2:30:00,  1.17it/s, loss=3.81, v_num=1]Epoch 0:   0%|          | 20/10549 [00:17<2:30:00,  1.17it/s, loss=3.8, v_num=1] Epoch 0:   0%|          | 21/10549 [00:17<2:28:39,  1.18it/s, loss=3.8, v_num=1]Epoch 0:   0%|          | 21/10549 [00:17<2:28:39,  1.18it/s, loss=3.9, v_num=1]Epoch 0:   0%|          | 22/10549 [00:18<2:29:20,  1.17it/s, loss=3.9, v_num=1]Epoch 0:   0%|          | 22/10549 [00:18<2:29:20,  1.17it/s, loss=3.95, v_num=1]Epoch 0:   0%|          | 23/10549 [00:19<2:28:10,  1.18it/s, loss=3.95, v_num=1]Epoch 0:   0%|          | 23/10549 [00:19<2:28:10,  1.18it/s, loss=3.94, v_num=1]Epoch 0:   0%|          | 24/10549 [00:21<2:35:45,  1.13it/s, loss=3.94, v_num=1]Epoch 0:   0%|          | 24/10549 [00:21<2:35:46,  1.13it/s, loss=4, v_num=1]   Epoch 0:   0%|          | 25/10549 [00:23<2:46:04,  1.06it/s, loss=4, v_num=1]Epoch 0:   0%|          | 25/10549 [00:23<2:46:04,  1.06it/s, loss=4.01, v_num=1]Epoch 0:   0%|          | 26/10549 [00:26<2:59:21,  1.02s/it, loss=4.01, v_num=1]Epoch 0:   0%|          | 26/10549 [00:26<2:59:21,  1.02s/it, loss=3.99, v_num=1]Epoch 0:   0%|          | 27/10549 [00:29<3:11:02,  1.09s/it, loss=3.99, v_num=1]Epoch 0:   0%|          | 27/10549 [00:29<3:11:02,  1.09s/it, loss=3.93, v_num=1]Epoch 0:   0%|          | 28/10549 [00:32<3:21:28,  1.15s/it, loss=3.93, v_num=1]Epoch 0:   0%|          | 28/10549 [00:32<3:21:28,  1.15s/it, loss=3.84, v_num=1]Epoch 0:   0%|          | 29/10549 [00:34<3:31:31,  1.21s/it, loss=3.84, v_num=1]Epoch 0:   0%|          | 29/10549 [00:34<3:31:31,  1.21s/it, loss=3.71, v_num=1]Epoch 0:   0%|          | 30/10549 [00:37<3:40:46,  1.26s/it, loss=3.71, v_num=1]Epoch 0:   0%|          | 30/10549 [00:37<3:40:46,  1.26s/it, loss=3.64, v_num=1]Epoch 0:   0%|          | 31/10549 [00:40<3:49:18,  1.31s/it, loss=3.64, v_num=1]Epoch 0:   0%|          | 31/10549 [00:40<3:49:18,  1.31s/it, loss=3.66, v_num=1]Epoch 0:   0%|          | 32/10549 [00:43<3:57:22,  1.35s/it, loss=3.66, v_num=1]Epoch 0:   0%|          | 32/10549 [00:43<3:57:22,  1.35s/it, loss=3.52, v_num=1]Epoch 0:   0%|          | 33/10549 [00:46<4:05:02,  1.40s/it, loss=3.52, v_num=1]Epoch 0:   0%|          | 33/10549 [00:46<4:05:02,  1.40s/it, loss=3.49, v_num=1]Epoch 0:   0%|          | 34/10549 [00:48<4:12:32,  1.44s/it, loss=3.49, v_num=1]Epoch 0:   0%|          | 34/10549 [00:48<4:12:32,  1.44s/it, loss=3.36, v_num=1]Epoch 0:   0%|          | 35/10549 [00:51<4:19:08,  1.48s/it, loss=3.36, v_num=1]Epoch 0:   0%|          | 35/10549 [00:51<4:19:08,  1.48s/it, loss=3.26, v_num=1]Epoch 0:   0%|          | 36/10549 [00:54<4:25:21,  1.51s/it, loss=3.26, v_num=1]Epoch 0:   0%|          | 36/10549 [00:54<4:25:21,  1.51s/it, loss=3.28, v_num=1]Epoch 0:   0%|          | 37/10549 [00:57<4:31:11,  1.55s/it, loss=3.28, v_num=1]Epoch 0:   0%|          | 37/10549 [00:57<4:31:11,  1.55s/it, loss=3.3, v_num=1] Epoch 0:   0%|          | 38/10549 [01:00<4:37:10,  1.58s/it, loss=3.3, v_num=1]Epoch 0:   0%|          | 38/10549 [01:00<4:37:10,  1.58s/it, loss=3.14, v_num=1]Epoch 0:   0%|          | 39/10549 [01:02<4:42:42,  1.61s/it, loss=3.14, v_num=1]Epoch 0:   0%|          | 39/10549 [01:02<4:42:42,  1.61s/it, loss=3.05, v_num=1]Epoch 0:   0%|          | 40/10549 [01:05<4:47:48,  1.64s/it, loss=3.05, v_num=1]Epoch 0:   0%|          | 40/10549 [01:05<4:47:48,  1.64s/it, loss=3.03, v_num=1]Epoch 0:   0%|          | 41/10549 [01:08<4:53:11,  1.67s/it, loss=3.03, v_num=1]Epoch 0:   0%|          | 41/10549 [01:08<4:53:11,  1.67s/it, loss=2.87, v_num=1]Epoch 0:   0%|          | 42/10549 [01:12<5:02:04,  1.72s/it, loss=2.87, v_num=1]Epoch 0:   0%|          | 42/10549 [01:12<5:02:04,  1.73s/it, loss=2.77, v_num=1]Epoch 0:   0%|          | 43/10549 [01:16<5:11:08,  1.78s/it, loss=2.77, v_num=1]Epoch 0:   0%|          | 43/10549 [01:16<5:11:08,  1.78s/it, loss=2.76, v_num=1]Epoch 0:   0%|          | 44/10549 [01:20<5:20:03,  1.83s/it, loss=2.76, v_num=1]Epoch 0:   0%|          | 44/10549 [01:20<5:20:03,  1.83s/it, loss=2.69, v_num=1]Epoch 0:   0%|          | 45/10549 [01:24<5:28:10,  1.87s/it, loss=2.69, v_num=1]Epoch 0:   0%|          | 45/10549 [01:24<5:28:10,  1.87s/it, loss=2.6, v_num=1] Epoch 0:   0%|          | 46/10549 [01:27<5:31:45,  1.90s/it, loss=2.6, v_num=1]Epoch 0:   0%|          | 46/10549 [01:27<5:31:45,  1.90s/it, loss=2.58, v_num=1]Epoch 0:   0%|          | 47/10549 [01:31<5:39:31,  1.94s/it, loss=2.58, v_num=1]Epoch 0:   0%|          | 47/10549 [01:31<5:39:31,  1.94s/it, loss=2.59, v_num=1]Epoch 0:   0%|          | 48/10549 [01:35<5:47:25,  1.99s/it, loss=2.59, v_num=1]Epoch 0:   0%|          | 48/10549 [01:35<5:47:25,  1.99s/it, loss=2.63, v_num=1]Epoch 0:   0%|          | 49/10549 [01:39<5:54:45,  2.03s/it, loss=2.63, v_num=1]Epoch 0:   0%|          | 49/10549 [01:39<5:54:45,  2.03s/it, loss=2.64, v_num=1]Epoch 0:   0%|          | 50/10549 [01:43<6:01:36,  2.07s/it, loss=2.64, v_num=1]Epoch 0:   0%|          | 50/10549 [01:43<6:01:36,  2.07s/it, loss=2.65, v_num=1]Epoch 0:   0%|          | 51/10549 [01:47<6:07:56,  2.10s/it, loss=2.65, v_num=1]Epoch 0:   0%|          | 51/10549 [01:47<6:07:56,  2.10s/it, loss=2.64, v_num=1]Epoch 0:   0%|          | 52/10549 [01:50<6:10:36,  2.12s/it, loss=2.64, v_num=1]Epoch 0:   0%|          | 52/10549 [01:50<6:10:36,  2.12s/it, loss=2.7, v_num=1] Epoch 0:   1%|          | 53/10549 [01:54<6:16:33,  2.15s/it, loss=2.7, v_num=1]Epoch 0:   1%|          | 53/10549 [01:54<6:16:33,  2.15s/it, loss=2.67, v_num=1]Epoch 0:   1%|          | 54/10549 [01:58<6:22:24,  2.19s/it, loss=2.67, v_num=1]Epoch 0:   1%|          | 54/10549 [01:58<6:22:24,  2.19s/it, loss=2.72, v_num=1]Epoch 0:   1%|          | 55/10549 [02:01<6:27:10,  2.21s/it, loss=2.72, v_num=1]Epoch 0:   1%|          | 55/10549 [02:01<6:27:10,  2.21s/it, loss=2.71, v_num=1]Epoch 0:   1%|          | 56/10549 [02:05<6:32:43,  2.25s/it, loss=2.71, v_num=1]Epoch 0:   1%|          | 56/10549 [02:05<6:32:43,  2.25s/it, loss=2.68, v_num=1]Epoch 0:   1%|          | 57/10549 [02:08<6:34:26,  2.26s/it, loss=2.68, v_num=1]Epoch 0:   1%|          | 57/10549 [02:08<6:34:26,  2.26s/it, loss=2.65, v_num=1]Epoch 0:   1%|          | 58/10549 [02:12<6:39:33,  2.29s/it, loss=2.65, v_num=1]Epoch 0:   1%|          | 58/10549 [02:12<6:39:34,  2.29s/it, loss=2.64, v_num=1]Epoch 0:   1%|          | 59/10549 [02:16<6:43:59,  2.31s/it, loss=2.64, v_num=1]Epoch 0:   1%|          | 59/10549 [02:16<6:43:59,  2.31s/it, loss=2.61, v_num=1]Epoch 0:   1%|          | 60/10549 [02:20<6:48:46,  2.34s/it, loss=2.61, v_num=1]Epoch 0:   1%|          | 60/10549 [02:20<6:48:46,  2.34s/it, loss=2.52, v_num=1]Epoch 0:   1%|          | 61/10549 [02:23<6:50:08,  2.35s/it, loss=2.52, v_num=1]Epoch 0:   1%|          | 61/10549 [02:23<6:50:08,  2.35s/it, loss=2.51, v_num=1]Epoch 0:   1%|          | 62/10549 [02:27<6:54:45,  2.37s/it, loss=2.51, v_num=1]Epoch 0:   1%|          | 62/10549 [02:27<6:54:45,  2.37s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 63/10549 [02:31<6:58:55,  2.40s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 63/10549 [02:31<6:58:55,  2.40s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 64/10549 [02:35<7:03:19,  2.42s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 64/10549 [02:35<7:03:19,  2.42s/it, loss=2.36, v_num=1]Epoch 0:   1%|          | 65/10549 [02:37<7:04:21,  2.43s/it, loss=2.36, v_num=1]Epoch 0:   1%|          | 65/10549 [02:37<7:04:21,  2.43s/it, loss=2.32, v_num=1]Epoch 0:   1%|          | 66/10549 [02:41<7:08:21,  2.45s/it, loss=2.32, v_num=1]Epoch 0:   1%|          | 66/10549 [02:41<7:08:21,  2.45s/it, loss=2.24, v_num=1]Epoch 0:   1%|          | 67/10549 [02:45<7:12:20,  2.47s/it, loss=2.24, v_num=1]Epoch 0:   1%|          | 67/10549 [02:45<7:12:20,  2.47s/it, loss=2.24, v_num=1]Epoch 0:   1%|          | 68/10549 [02:49<7:15:35,  2.49s/it, loss=2.24, v_num=1]Epoch 0:   1%|          | 68/10549 [02:49<7:15:36,  2.49s/it, loss=2.19, v_num=1]Epoch 0:   1%|          | 69/10549 [02:53<7:19:19,  2.52s/it, loss=2.19, v_num=1]Epoch 0:   1%|          | 69/10549 [02:53<7:19:19,  2.52s/it, loss=2.2, v_num=1] Epoch 0:   1%|          | 70/10549 [02:56<7:20:01,  2.52s/it, loss=2.2, v_num=1]Epoch 0:   1%|          | 70/10549 [02:56<7:20:01,  2.52s/it, loss=2.14, v_num=1]Epoch 0:   1%|          | 71/10549 [03:00<7:23:20,  2.54s/it, loss=2.14, v_num=1]Epoch 0:   1%|          | 71/10549 [03:00<7:23:20,  2.54s/it, loss=2.12, v_num=1]Epoch 0:   1%|          | 72/10549 [03:04<7:26:43,  2.56s/it, loss=2.12, v_num=1]Epoch 0:   1%|          | 72/10549 [03:04<7:26:43,  2.56s/it, loss=2.13, v_num=1]Epoch 0:   1%|          | 73/10549 [03:08<7:30:05,  2.58s/it, loss=2.13, v_num=1]Epoch 0:   1%|          | 73/10549 [03:08<7:30:05,  2.58s/it, loss=2.18, v_num=1]Epoch 0:   1%|          | 74/10549 [03:11<7:30:40,  2.58s/it, loss=2.18, v_num=1]Epoch 0:   1%|          | 74/10549 [03:11<7:30:40,  2.58s/it, loss=2.17, v_num=1]Epoch 0:   1%|          | 75/10549 [03:14<7:33:39,  2.60s/it, loss=2.17, v_num=1]Epoch 0:   1%|          | 75/10549 [03:14<7:33:39,  2.60s/it, loss=2.17, v_num=1]Epoch 0:   1%|          | 76/10549 [03:18<7:36:41,  2.62s/it, loss=2.17, v_num=1]Epoch 0:   1%|          | 76/10549 [03:18<7:36:41,  2.62s/it, loss=2.09, v_num=1]Epoch 0:   1%|          | 77/10549 [03:22<7:39:06,  2.63s/it, loss=2.09, v_num=1]Epoch 0:   1%|          | 77/10549 [03:22<7:39:07,  2.63s/it, loss=2, v_num=1]   Epoch 0:   1%|          | 78/10549 [03:26<7:42:01,  2.65s/it, loss=2, v_num=1]Epoch 0:   1%|          | 78/10549 [03:26<7:42:01,  2.65s/it, loss=1.98, v_num=1]Epoch 0:   1%|          | 79/10549 [03:29<7:42:29,  2.65s/it, loss=1.98, v_num=1]Epoch 0:   1%|          | 79/10549 [03:29<7:42:29,  2.65s/it, loss=1.96, v_num=1]Epoch 0:   1%|          | 80/10549 [03:33<7:44:52,  2.66s/it, loss=1.96, v_num=1]Epoch 0:   1%|          | 80/10549 [03:33<7:44:52,  2.66s/it, loss=1.94, v_num=1]Epoch 0:   1%|          | 81/10549 [03:37<7:47:32,  2.68s/it, loss=1.94, v_num=1]Epoch 0:   1%|          | 81/10549 [03:37<7:47:32,  2.68s/it, loss=1.92, v_num=1]Epoch 0:   1%|          | 82/10549 [03:40<7:50:09,  2.70s/it, loss=1.92, v_num=1]Epoch 0:   1%|          | 82/10549 [03:40<7:50:09,  2.70s/it, loss=1.98, v_num=1]Epoch 0:   1%|          | 83/10549 [03:43<7:50:27,  2.70s/it, loss=1.98, v_num=1]Epoch 0:   1%|          | 83/10549 [03:43<7:50:27,  2.70s/it, loss=1.92, v_num=1]Epoch 0:   1%|          | 84/10549 [03:47<7:52:54,  2.71s/it, loss=1.92, v_num=1]Epoch 0:   1%|          | 84/10549 [03:47<7:52:54,  2.71s/it, loss=1.92, v_num=1]Epoch 0:   1%|          | 85/10549 [03:51<7:55:25,  2.73s/it, loss=1.92, v_num=1]Epoch 0:   1%|          | 85/10549 [03:51<7:55:25,  2.73s/it, loss=1.94, v_num=1]Epoch 0:   1%|          | 86/10549 [03:55<7:57:49,  2.74s/it, loss=1.94, v_num=1]Epoch 0:   1%|          | 86/10549 [03:55<7:57:49,  2.74s/it, loss=2.04, v_num=1]Epoch 0:   1%|          | 87/10549 [03:59<8:00:26,  2.76s/it, loss=2.04, v_num=1]Epoch 0:   1%|          | 87/10549 [03:59<8:00:26,  2.76s/it, loss=1.99, v_num=1]Epoch 0:   1%|          | 88/10549 [04:03<8:02:50,  2.77s/it, loss=1.99, v_num=1]Epoch 0:   1%|          | 88/10549 [04:03<8:02:50,  2.77s/it, loss=2.02, v_num=1]Epoch 0:   1%|          | 89/10549 [04:07<8:05:02,  2.78s/it, loss=2.02, v_num=1]Epoch 0:   1%|          | 89/10549 [04:07<8:05:02,  2.78s/it, loss=2.04, v_num=1]Epoch 0:   1%|          | 90/10549 [04:10<8:05:12,  2.78s/it, loss=2.04, v_num=1]Epoch 0:   1%|          | 90/10549 [04:10<8:05:12,  2.78s/it, loss=2.06, v_num=1]Epoch 0:   1%|          | 91/10549 [04:14<8:07:23,  2.80s/it, loss=2.06, v_num=1]Epoch 0:   1%|          | 91/10549 [04:14<8:07:23,  2.80s/it, loss=2, v_num=1]   Epoch 0:   1%|          | 92/10549 [04:18<8:09:33,  2.81s/it, loss=2, v_num=1]Epoch 0:   1%|          | 92/10549 [04:18<8:09:33,  2.81s/it, loss=2.02, v_num=1]Epoch 0:   1%|          | 93/10549 [04:22<8:11:35,  2.82s/it, loss=2.02, v_num=1]Epoch 0:   1%|          | 93/10549 [04:22<8:11:35,  2.82s/it, loss=1.93, v_num=1]Epoch 0:   1%|          | 94/10549 [04:26<8:13:43,  2.83s/it, loss=1.93, v_num=1]Epoch 0:   1%|          | 94/10549 [04:26<8:13:43,  2.83s/it, loss=1.91, v_num=1]Epoch 0:   1%|          | 95/10549 [04:29<8:13:45,  2.83s/it, loss=1.91, v_num=1]Epoch 0:   1%|          | 95/10549 [04:29<8:13:45,  2.83s/it, loss=2.02, v_num=1]Epoch 0:   1%|          | 96/10549 [04:33<8:15:54,  2.85s/it, loss=2.02, v_num=1]Epoch 0:   1%|          | 96/10549 [04:33<8:15:54,  2.85s/it, loss=2.1, v_num=1] Epoch 0:   1%|          | 97/10549 [04:37<8:17:51,  2.86s/it, loss=2.1, v_num=1]Epoch 0:   1%|          | 97/10549 [04:37<8:17:51,  2.86s/it, loss=2.09, v_num=1]Epoch 0:   1%|          | 98/10549 [04:41<8:19:44,  2.87s/it, loss=2.09, v_num=1]Epoch 0:   1%|          | 98/10549 [04:41<8:19:44,  2.87s/it, loss=2.08, v_num=1]Epoch 0:   1%|          | 99/10549 [04:44<8:19:41,  2.87s/it, loss=2.08, v_num=1]Epoch 0:   1%|          | 99/10549 [04:44<8:19:41,  2.87s/it, loss=2.21, v_num=1]Epoch 0:   1%|          | 100/10549 [04:47<8:21:26,  2.88s/it, loss=2.21, v_num=1]Epoch 0:   1%|          | 100/10549 [04:47<8:21:26,  2.88s/it, loss=2.3, v_num=1] Epoch 0:   1%|          | 101/10549 [04:51<8:23:21,  2.89s/it, loss=2.3, v_num=1]Epoch 0:   1%|          | 101/10549 [04:51<8:23:21,  2.89s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 102/10549 [04:55<8:25:02,  2.90s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 102/10549 [04:55<8:25:02,  2.90s/it, loss=2.43, v_num=1]Epoch 0:   1%|          | 103/10549 [04:59<8:26:59,  2.91s/it, loss=2.43, v_num=1]Epoch 0:   1%|          | 103/10549 [04:59<8:26:59,  2.91s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 104/10549 [05:02<8:26:48,  2.91s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 104/10549 [05:02<8:26:48,  2.91s/it, loss=2.54, v_num=1]Epoch 0:   1%|          | 105/10549 [05:06<8:28:31,  2.92s/it, loss=2.54, v_num=1]Epoch 0:   1%|          | 105/10549 [05:06<8:28:31,  2.92s/it, loss=2.52, v_num=1]Epoch 0:   1%|          | 106/10549 [05:10<8:30:22,  2.93s/it, loss=2.52, v_num=1]Epoch 0:   1%|          | 106/10549 [05:10<8:30:22,  2.93s/it, loss=2.51, v_num=1]Epoch 0:   1%|          | 107/10549 [05:14<8:31:58,  2.94s/it, loss=2.51, v_num=1]Epoch 0:   1%|          | 107/10549 [05:14<8:31:58,  2.94s/it, loss=2.48, v_num=1]Epoch 0:   1%|          | 108/10549 [05:17<8:31:47,  2.94s/it, loss=2.48, v_num=1]Epoch 0:   1%|          | 108/10549 [05:17<8:31:47,  2.94s/it, loss=2.43, v_num=1]Epoch 0:   1%|          | 109/10549 [05:21<8:33:22,  2.95s/it, loss=2.43, v_num=1]Epoch 0:   1%|          | 109/10549 [05:21<8:33:22,  2.95s/it, loss=2.48, v_num=1]Epoch 0:   1%|          | 110/10549 [05:25<8:35:03,  2.96s/it, loss=2.48, v_num=1]Epoch 0:   1%|          | 110/10549 [05:25<8:35:03,  2.96s/it, loss=2.54, v_num=1]Epoch 0:   1%|          | 111/10549 [05:29<8:36:36,  2.97s/it, loss=2.54, v_num=1]Epoch 0:   1%|          | 111/10549 [05:29<8:36:36,  2.97s/it, loss=2.59, v_num=1]Epoch 0:   1%|          | 112/10549 [05:33<8:37:59,  2.98s/it, loss=2.59, v_num=1]Epoch 0:   1%|          | 112/10549 [05:33<8:37:59,  2.98s/it, loss=2.46, v_num=1]Epoch 0:   1%|          | 113/10549 [05:36<8:37:45,  2.98s/it, loss=2.46, v_num=1]Epoch 0:   1%|          | 113/10549 [05:36<8:37:45,  2.98s/it, loss=2.56, v_num=1]Epoch 0:   1%|          | 114/10549 [05:40<8:39:06,  2.98s/it, loss=2.56, v_num=1]Epoch 0:   1%|          | 114/10549 [05:40<8:39:06,  2.98s/it, loss=2.54, v_num=1]Epoch 0:   1%|          | 115/10549 [05:44<8:40:44,  2.99s/it, loss=2.54, v_num=1]Epoch 0:   1%|          | 115/10549 [05:44<8:40:44,  2.99s/it, loss=2.53, v_num=1]Epoch 0:   1%|          | 116/10549 [05:48<8:42:07,  3.00s/it, loss=2.53, v_num=1]Epoch 0:   1%|          | 116/10549 [05:48<8:42:07,  3.00s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 117/10549 [05:52<8:43:25,  3.01s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 117/10549 [05:52<8:43:25,  3.01s/it, loss=2.5, v_num=1] Epoch 0:   1%|          | 118/10549 [05:55<8:43:10,  3.01s/it, loss=2.5, v_num=1]Epoch 0:   1%|          | 118/10549 [05:55<8:43:10,  3.01s/it, loss=2.58, v_num=1]Epoch 0:   1%|          | 119/10549 [05:59<8:44:28,  3.02s/it, loss=2.58, v_num=1]Epoch 0:   1%|          | 119/10549 [05:59<8:44:28,  3.02s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 120/10549 [06:03<8:45:50,  3.03s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 120/10549 [06:03<8:45:50,  3.03s/it, loss=2.4, v_num=1] Epoch 0:   1%|          | 121/10549 [06:07<8:47:12,  3.03s/it, loss=2.4, v_num=1]Epoch 0:   1%|          | 121/10549 [06:07<8:47:12,  3.03s/it, loss=2.41, v_num=1]Epoch 0:   1%|          | 122/10549 [06:10<8:47:04,  3.03s/it, loss=2.41, v_num=1]Epoch 0:   1%|          | 122/10549 [06:10<8:47:04,  3.03s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 123/10549 [06:13<8:48:12,  3.04s/it, loss=2.42, v_num=1]Epoch 0:   1%|          | 123/10549 [06:13<8:48:13,  3.04s/it, loss=2.49, v_num=1]Epoch 0:   1%|          | 124/10549 [06:18<8:49:41,  3.05s/it, loss=2.49, v_num=1]Epoch 0:   1%|          | 124/10549 [06:18<8:49:41,  3.05s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 125/10549 [06:22<8:51:03,  3.06s/it, loss=2.47, v_num=1]Epoch 0:   1%|          | 125/10549 [06:22<8:51:03,  3.06s/it, loss=2.61, v_num=1]Epoch 0:   1%|          | 126/10549 [06:26<8:52:16,  3.06s/it, loss=2.61, v_num=1]Epoch 0:   1%|          | 126/10549 [06:26<8:52:16,  3.06s/it, loss=2.62, v_num=1]Epoch 0:   1%|          | 127/10549 [06:28<8:51:52,  3.06s/it, loss=2.62, v_num=1]Epoch 0:   1%|          | 127/10549 [06:28<8:51:52,  3.06s/it, loss=2.7, v_num=1] Epoch 0:   1%|          | 128/10549 [06:32<8:53:05,  3.07s/it, loss=2.7, v_num=1]Epoch 0:   1%|          | 128/10549 [06:32<8:53:05,  3.07s/it, loss=2.76, v_num=1]Epoch 0:   1%|          | 129/10549 [06:36<8:54:16,  3.08s/it, loss=2.76, v_num=1]Epoch 0:   1%|          | 129/10549 [06:36<8:54:16,  3.08s/it, loss=2.69, v_num=1]Epoch 0:   1%|          | 130/10549 [06:40<8:55:25,  3.08s/it, loss=2.69, v_num=1]Epoch 0:   1%|          | 130/10549 [06:40<8:55:25,  3.08s/it, loss=2.64, v_num=1]Epoch 0:   1%|          | 131/10549 [06:44<8:56:34,  3.09s/it, loss=2.64, v_num=1]Epoch 0:   1%|          | 131/10549 [06:44<8:56:34,  3.09s/it, loss=2.65, v_num=1]Epoch 0:   1%|▏         | 132/10549 [06:47<8:56:11,  3.09s/it, loss=2.65, v_num=1]Epoch 0:   1%|▏         | 132/10549 [06:47<8:56:11,  3.09s/it, loss=2.78, v_num=1]Epoch 0:   1%|▏         | 133/10549 [06:51<8:57:15,  3.09s/it, loss=2.78, v_num=1]Epoch 0:   1%|▏         | 133/10549 [06:51<8:57:15,  3.09s/it, loss=2.71, v_num=1]Epoch 0:   1%|▏         | 134/10549 [06:55<8:58:21,  3.10s/it, loss=2.71, v_num=1]Epoch 0:   1%|▏         | 134/10549 [06:55<8:58:21,  3.10s/it, loss=2.79, v_num=1]Epoch 0:   1%|▏         | 135/10549 [06:59<8:59:27,  3.11s/it, loss=2.79, v_num=1]Epoch 0:   1%|▏         | 135/10549 [06:59<8:59:27,  3.11s/it, loss=2.72, v_num=1]Epoch 0:   1%|▏         | 136/10549 [07:03<9:00:26,  3.11s/it, loss=2.72, v_num=1]Epoch 0:   1%|▏         | 136/10549 [07:03<9:00:26,  3.11s/it, loss=2.77, v_num=1]Epoch 0:   1%|▏         | 137/10549 [07:06<9:00:07,  3.11s/it, loss=2.77, v_num=1]Epoch 0:   1%|▏         | 137/10549 [07:06<9:00:07,  3.11s/it, loss=2.74, v_num=1]Epoch 0:   1%|▏         | 138/10549 [07:10<9:01:18,  3.12s/it, loss=2.74, v_num=1]Epoch 0:   1%|▏         | 138/10549 [07:10<9:01:18,  3.12s/it, loss=2.7, v_num=1] Epoch 0:   1%|▏         | 139/10549 [07:14<9:02:19,  3.13s/it, loss=2.7, v_num=1]Epoch 0:   1%|▏         | 139/10549 [07:14<9:02:19,  3.13s/it, loss=2.85, v_num=1]Epoch 0:   1%|▏         | 140/10549 [07:18<9:03:21,  3.13s/it, loss=2.85, v_num=1]Epoch 0:   1%|▏         | 140/10549 [07:18<9:03:21,  3.13s/it, loss=2.91, v_num=1]Epoch 0:   1%|▏         | 141/10549 [07:22<9:04:17,  3.14s/it, loss=2.91, v_num=1]Epoch 0:   1%|▏         | 141/10549 [07:22<9:04:17,  3.14s/it, loss=2.87, v_num=1]Epoch 0:   1%|▏         | 142/10549 [07:25<9:03:59,  3.14s/it, loss=2.87, v_num=1]Epoch 0:   1%|▏         | 142/10549 [07:25<9:03:59,  3.14s/it, loss=2.79, v_num=1]Epoch 0:   1%|▏         | 143/10549 [07:29<9:05:03,  3.14s/it, loss=2.79, v_num=1]Epoch 0:   1%|▏         | 143/10549 [07:29<9:05:03,  3.14s/it, loss=2.64, v_num=1]Epoch 0:   1%|▏         | 144/10549 [07:33<9:06:16,  3.15s/it, loss=2.64, v_num=1]Epoch 0:   1%|▏         | 144/10549 [07:33<9:06:16,  3.15s/it, loss=2.68, v_num=1]Epoch 0:   1%|▏         | 145/10549 [07:37<9:07:04,  3.16s/it, loss=2.68, v_num=1]Epoch 0:   1%|▏         | 145/10549 [07:37<9:07:04,  3.16s/it, loss=2.56, v_num=1]Epoch 0:   1%|▏         | 146/10549 [07:41<9:08:01,  3.16s/it, loss=2.56, v_num=1]Epoch 0:   1%|▏         | 146/10549 [07:41<9:08:01,  3.16s/it, loss=2.55, v_num=1]Epoch 0:   1%|▏         | 147/10549 [07:44<9:07:37,  3.16s/it, loss=2.55, v_num=1]Epoch 0:   1%|▏         | 147/10549 [07:44<9:07:37,  3.16s/it, loss=2.58, v_num=1]Epoch 0:   1%|▏         | 148/10549 [07:48<9:08:26,  3.16s/it, loss=2.58, v_num=1]Epoch 0:   1%|▏         | 148/10549 [07:48<9:08:26,  3.16s/it, loss=2.6, v_num=1] Epoch 0:   1%|▏         | 149/10549 [07:51<9:08:58,  3.17s/it, loss=2.6, v_num=1]Epoch 0:   1%|▏         | 149/10549 [07:51<9:08:58,  3.17s/it, loss=2.6, v_num=1]Traceback (most recent call last):
  File "/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py", line 277, in <module>
    main()
  File "/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py", line 250, in main
    trainer.fit(model, data_model)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1283, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 271, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 358, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1550, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/core/module.py", line 1705, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 289, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 216, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/deepspeed.py", line 111, in optimizer_step
    closure_result = closure()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 141, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 304, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1704, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 191, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/deepspeed.py", line 90, in backward
    deepspeed_engine.backward(closure_loss, *args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1799, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1953, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 51, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 1; 39.59 GiB total capacity; 34.82 GiB already allocated; 1.47 GiB free; 35.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
./fintune_bloomz_mt_lowsub_highinter.sh: line 146: 58431 Killed                  python $CMD
++ date +%s
+ run_ts=1671235522
++ run_ts
./fintune_bloomz_mt_lowsub_highinter.sh: line 16: run_ts: command not found
+ echo 'RUN TS: '
RUN TS: 
++ date
+ echo 'START TIME: Sat Dec 17 08:05:22 CST 2022'
START TIME: Sat Dec 17 08:05:22 CST 2022
+ MICRO_BATCH_SIZE=1
+ ROOT_DIR=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid
+ '[' '!' -d /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid ']'
+ echo /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid 'exist!!!!!!!!!!!!!!!'
/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid exist!!!!!!!!!!!!!!!
+ ZERO_STAGE=2
+ config_json=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/training_config.json
+ export MASTER_PORT=32160
+ MASTER_PORT=32160
+ cat
+ export PL_DEEPSPEED_CONFIG_PATH=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/training_config.json
+ PL_DEEPSPEED_CONFIG_PATH=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/training_config.json
+ checkpoint_path=/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522
+ export TORCH_EXTENSIONS_DIR=/home/ubuntu/cloudfs/torch_extendsions
+ TORCH_EXTENSIONS_DIR=/home/ubuntu/cloudfs/torch_extendsions
+ TRAINER_ARGS='
    --max_epochs 50     --gpus 8     --num_nodes 1     --strategy deepspeed_stage_2     --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid     --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522     --save_top_k 2     --monitor train_loss     --mode min     --save_last     --check_val_every_n_epoch 1 '
+ DATA_DIR=/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/
+ train_data_file=/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv
+ val_data_file=/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv
+ DATA_ARGS='
    --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/     --train_batchsize 1     --valid_batchsize 1     --train_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv     --valid_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --test_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --max_seq_length 700 '
+ PRETRAINED_MODEL_PATH=/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt
+ MODEL_ARGS='
    --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt     --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json     --learning_rate 4e-5     --weight_decay 0.1     --warmup 0.01     --run_ts 1671235522 '
+ SCRIPTS_PATH=/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py
+ export 'CMD=     /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py     
    --max_epochs 50     --gpus 8     --num_nodes 1     --strategy deepspeed_stage_2     --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid     --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522     --save_top_k 2     --monitor train_loss     --mode min     --save_last     --check_val_every_n_epoch 1      
    --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt     --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json     --learning_rate 4e-5     --weight_decay 0.1     --warmup 0.01     --run_ts 1671235522      
    --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/     --train_batchsize 1     --valid_batchsize 1     --train_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv     --valid_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --test_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --max_seq_length 700      '
+ CMD='     /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py     
    --max_epochs 50     --gpus 8     --num_nodes 1     --strategy deepspeed_stage_2     --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid     --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522     --save_top_k 2     --monitor train_loss     --mode min     --save_last     --check_val_every_n_epoch 1      
    --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt     --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json     --learning_rate 4e-5     --weight_decay 0.1     --warmup 0.01     --run_ts 1671235522      
    --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/     --train_batchsize 1     --valid_batchsize 1     --train_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv     --valid_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --test_data  /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv     --max_seq_length 700      '
+ echo /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py --max_epochs 50 --gpus 8 --num_nodes 1 --strategy deepspeed_stage_2 --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522 --save_top_k 2 --monitor train_loss --mode min --save_last --check_val_every_n_epoch 1 --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json --learning_rate 4e-5 --weight_decay 0.1 --warmup 0.01 --run_ts 1671235522 --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/ --train_batchsize 1 --valid_batchsize 1 --train_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv --valid_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --test_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --max_seq_length 700
/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py --max_epochs 50 --gpus 8 --num_nodes 1 --strategy deepspeed_stage_2 --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522 --save_top_k 2 --monitor train_loss --mode min --save_last --check_val_every_n_epoch 1 --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json --learning_rate 4e-5 --weight_decay 0.1 --warmup 0.01 --run_ts 1671235522 --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/ --train_batchsize 1 --valid_batchsize 1 --train_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv --valid_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --test_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --max_seq_length 700
+ python /home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py --max_epochs 50 --gpus 8 --num_nodes 1 --strategy deepspeed_stage_2 --default_root_dir /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid --dirpath /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522 --save_top_k 2 --monitor train_loss --mode min --save_last --check_val_every_n_epoch 1 --pretrained_model_path /home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt --output_save_path /home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json --learning_rate 4e-5 --weight_decay 0.1 --warmup 0.01 --run_ts 1671235522 --data_dir /home/ubuntu/cloudfs/ghost_data/merge_all_title_content/ --train_batchsize 1 --valid_batchsize 1 --train_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv --valid_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --test_data /home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv --max_seq_length 700
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=8)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=8)` instead.
  rank_zero_deprecation(
Loading DeepSpeed config from set PL_DEEPSPEED_CONFIG_PATH environment variable
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 3, MEMBER: 4/8
initializing deepspeed distributed: GLOBAL_RANK: 5, MEMBER: 6/8
initializing deepspeed distributed: GLOBAL_RANK: 4, MEMBER: 5/8
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
['/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python38.zip', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/lib-dynload', '/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages', '../..']
args:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=None, check_val_every_n_epoch=1, data_dir='/home/ubuntu/cloudfs/ghost_data/merge_all_title_content/', default_root_dir='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid', detect_anomaly=False, deterministic=None, devices=None, dirpath='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/ckpt_1671235522', do_eval_only=False, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, every_n_train_steps=1000, fast_dev_run=False, filename='model-{epoch:02d}-{train_loss:.4f}', gpus=8, gradient_clip_algorithm=None, gradient_clip_val=None, ipus=None, learning_rate=4e-05, limit_predict_batches=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=50, max_seq_length=700, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, mode='min', monitor='train_loss', move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=None, num_sanity_val_steps=2, num_workers=2, output_save_path='/home/ubuntu/cloudfs/saved_models/deep_speed_experiments/gpt2/make_friends_live_streaming_guid/predict.json', overfit_batches=0.0, plugins=None, precision=32, pretrained_model_path='/home/ubuntu/cloudfs/saved_models/bigscience/bloomz-7b1-mt', profiler=None, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, run_ts='1671235522', save_last=True, save_top_k=2.0, save_weights_only=True, strategy='deepspeed_stage_2', sync_batchnorm=False, test_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', tpu_cores=None, track_grad_norm=-1, train_batchsize=1, train_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_train_1208_1670534928.csv', val_check_interval=None, valid_batchsize=1, valid_data='/home/ubuntu/cloudfs/ghost_data/newred_redbook_link_download//redbook_tags_content_to_title_sim_tags_training_data_val_1208_1670534928.csv', warmup=0.01, weight_decay=0.1, weights_save_path=None)
num_data: 83541
initializing deepspeed distributed: GLOBAL_RANK: 6, MEMBER: 7/8
initializing deepspeed distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
Total training step: 522131
/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2182: LightningDeprecationWarning: `Trainer.gpus` was deprecated in v1.6 and will be removed in v1.8. Please use `Trainer.num_devices` or `Trainer.device_ids` to get device information instead.
  rank_zero_deprecation(
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
You have specified an optimizer and/or scheduler within the DeepSpeed config. It is recommended to define it in `LightningModule.configure_optimizers`.

  | Name  | Type             | Params
-------------------------------------------
0 | model | BloomForCausalLM | 7.1 B 
-------------------------------------------
7.1 B     Trainable params
0         Non-trainable params
7.1 B     Total params
28,276.064Total estimated model params size (MB)
Rank: 0 partition count [8] and sizes[(883627008, False)] 
Sanity Checking: 0it [00:00, ?it/s]/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 164 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Rank: 4 partition count [8] and sizes[(883627008, False)] 
Rank: 2 partition count [8] and sizes[(883627008, False)] 
Rank: 1 partition count [8] and sizes[(883627008, False)] 
Rank: 6 partition count [8] and sizes[(883627008, False)] 
Rank: 7 partition count [8] and sizes[(883627008, False)] 
Rank: 5 partition count [8] and sizes[(883627008, False)] 
Rank: 3 partition count [8] and sizes[(883627008, False)] 
Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  2.42s/it]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]validation_epoch_end...
                                                                           /home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
  warning_cache.warn(
/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 164 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/10549 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/10549 [00:00<?, ?it/s] validation_epoch_end...
validation_epoch_end...
validation_epoch_end...
validation_epoch_end...
validation_epoch_end...
validation_epoch_end...
validation_epoch_end...
Epoch 0:   0%|          | 1/10549 [00:03<9:51:29,  3.36s/it]Epoch 0:   0%|          | 1/10549 [00:03<9:51:35,  3.37s/it, loss=4.59, v_num=2]Epoch 0:   0%|          | 2/10549 [00:03<5:50:08,  1.99s/it, loss=4.59, v_num=2]Epoch 0:   0%|          | 2/10549 [00:03<5:50:10,  1.99s/it, loss=4.81, v_num=2]Epoch 0:   0%|          | 3/10549 [00:05<5:39:59,  1.93s/it, loss=4.81, v_num=2]Epoch 0:   0%|          | 3/10549 [00:05<5:40:00,  1.93s/it, loss=4.49, v_num=2]Epoch 0:   0%|          | 4/10549 [00:06<4:50:59,  1.66s/it, loss=4.49, v_num=2]Epoch 0:   0%|          | 4/10549 [00:06<4:51:00,  1.66s/it, loss=4.38, v_num=2]Epoch 0:   0%|          | 5/10549 [00:07<4:17:29,  1.47s/it, loss=4.38, v_num=2]Epoch 0:   0%|          | 5/10549 [00:07<4:17:30,  1.47s/it, loss=4.44, v_num=2]Epoch 0:   0%|          | 6/10549 [00:09<4:29:33,  1.53s/it, loss=4.44, v_num=2]Epoch 0:   0%|          | 6/10549 [00:09<4:29:34,  1.53s/it, loss=4.42, v_num=2]Epoch 0:   0%|          | 7/10549 [00:09<4:08:46,  1.42s/it, loss=4.42, v_num=2]Epoch 0:   0%|          | 7/10549 [00:09<4:08:47,  1.42s/it, loss=4.4, v_num=2] Epoch 0:   0%|          | 8/10549 [00:11<4:17:37,  1.47s/it, loss=4.4, v_num=2]Epoch 0:   0%|          | 8/10549 [00:11<4:17:38,  1.47s/it, loss=4.32, v_num=2]Epoch 0:   0%|          | 9/10549 [00:12<4:02:28,  1.38s/it, loss=4.32, v_num=2]Epoch 0:   0%|          | 9/10549 [00:12<4:02:28,  1.38s/it, loss=4.31, v_num=2]Epoch 0:   0%|          | 10/10549 [00:13<3:53:21,  1.33s/it, loss=4.31, v_num=2]Epoch 0:   0%|          | 10/10549 [00:13<3:53:22,  1.33s/it, loss=4.39, v_num=2]Epoch 0:   0%|          | 11/10549 [00:15<4:00:00,  1.37s/it, loss=4.39, v_num=2]Epoch 0:   0%|          | 11/10549 [00:15<4:00:01,  1.37s/it, loss=4.36, v_num=2]Epoch 0:   0%|          | 12/10549 [00:15<3:52:25,  1.32s/it, loss=4.36, v_num=2]Epoch 0:   0%|          | 12/10549 [00:15<3:52:25,  1.32s/it, loss=4.37, v_num=2]Epoch 0:   0%|          | 13/10549 [00:16<3:43:53,  1.28s/it, loss=4.37, v_num=2]Epoch 0:   0%|          | 13/10549 [00:16<3:43:54,  1.28s/it, loss=4.28, v_num=2]Epoch 0:   0%|          | 14/10549 [00:18<3:52:14,  1.32s/it, loss=4.28, v_num=2]Epoch 0:   0%|          | 14/10549 [00:18<3:52:14,  1.32s/it, loss=4.3, v_num=2] Epoch 0:   0%|          | 15/10549 [00:19<3:44:49,  1.28s/it, loss=4.3, v_num=2]Epoch 0:   0%|          | 15/10549 [00:19<3:44:50,  1.28s/it, loss=4.33, v_num=2]Epoch 0:   0%|          | 16/10549 [00:20<3:40:20,  1.26s/it, loss=4.33, v_num=2]Epoch 0:   0%|          | 16/10549 [00:20<3:40:20,  1.26s/it, loss=4.33, v_num=2]Epoch 0:   0%|          | 17/10549 [00:21<3:44:40,  1.28s/it, loss=4.33, v_num=2]Epoch 0:   0%|          | 17/10549 [00:21<3:44:40,  1.28s/it, loss=4.32, v_num=2]Epoch 0:   0%|          | 18/10549 [00:22<3:40:41,  1.26s/it, loss=4.32, v_num=2]Epoch 0:   0%|          | 18/10549 [00:22<3:40:41,  1.26s/it, loss=4.31, v_num=2]Epoch 0:   0%|          | 19/10549 [00:23<3:35:30,  1.23s/it, loss=4.31, v_num=2]Epoch 0:   0%|          | 19/10549 [00:23<3:35:31,  1.23s/it, loss=4.27, v_num=2]Epoch 0:   0%|          | 20/10549 [00:25<3:40:41,  1.26s/it, loss=4.27, v_num=2]Epoch 0:   0%|          | 20/10549 [00:25<3:40:42,  1.26s/it, loss=4.29, v_num=2]Epoch 0:   0%|          | 21/10549 [00:25<3:36:00,  1.23s/it, loss=4.29, v_num=2]Epoch 0:   0%|          | 21/10549 [00:25<3:36:00,  1.23s/it, loss=4.27, v_num=2]Epoch 0:   0%|          | 22/10549 [00:27<3:42:15,  1.27s/it, loss=4.27, v_num=2]Epoch 0:   0%|          | 22/10549 [00:27<3:42:16,  1.27s/it, loss=4.28, v_num=2]Epoch 0:   0%|          | 23/10549 [00:28<3:37:56,  1.24s/it, loss=4.28, v_num=2]Epoch 0:   0%|          | 23/10549 [00:28<3:37:57,  1.24s/it, loss=4.29, v_num=2]Epoch 0:   0%|          | 24/10549 [00:30<3:42:56,  1.27s/it, loss=4.29, v_num=2]Epoch 0:   0%|          | 24/10549 [00:30<3:42:57,  1.27s/it, loss=4.33, v_num=2]Epoch 0:   0%|          | 25/10549 [00:33<3:57:49,  1.36s/it, loss=4.33, v_num=2]Epoch 0:   0%|          | 25/10549 [00:33<3:57:49,  1.36s/it, loss=4.28, v_num=2]Epoch 0:   0%|          | 26/10549 [00:38<4:16:26,  1.46s/it, loss=4.28, v_num=2]Epoch 0:   0%|          | 26/10549 [00:38<4:16:26,  1.46s/it, loss=4.25, v_num=2]Epoch 0:   0%|          | 27/10549 [00:41<4:31:39,  1.55s/it, loss=4.25, v_num=2]Epoch 0:   0%|          | 27/10549 [00:41<4:31:39,  1.55s/it, loss=4.17, v_num=2]Epoch 0:   0%|          | 28/10549 [00:45<4:46:23,  1.63s/it, loss=4.17, v_num=2]Epoch 0:   0%|          | 28/10549 [00:45<4:46:23,  1.63s/it, loss=4.08, v_num=2]Epoch 0:   0%|          | 29/10549 [00:48<4:53:44,  1.68s/it, loss=4.08, v_num=2]Epoch 0:   0%|          | 29/10549 [00:48<4:53:44,  1.68s/it, loss=4.07, v_num=2]Epoch 0:   0%|          | 30/10549 [00:52<5:06:31,  1.75s/it, loss=4.07, v_num=2]Epoch 0:   0%|          | 30/10549 [00:52<5:06:31,  1.75s/it, loss=4, v_num=2]   Epoch 0:   0%|          | 31/10549 [00:56<5:19:08,  1.82s/it, loss=4, v_num=2]Epoch 0:   0%|          | 31/10549 [00:56<5:19:08,  1.82s/it, loss=3.85, v_num=2]Epoch 0:   0%|          | 32/10549 [01:00<5:31:10,  1.89s/it, loss=3.85, v_num=2]Epoch 0:   0%|          | 32/10549 [01:00<5:31:10,  1.89s/it, loss=3.78, v_num=2]Epoch 0:   0%|          | 33/10549 [01:04<5:41:51,  1.95s/it, loss=3.78, v_num=2]Epoch 0:   0%|          | 33/10549 [01:04<5:41:51,  1.95s/it, loss=3.69, v_num=2]Epoch 0:   0%|          | 34/10549 [01:07<5:46:26,  1.98s/it, loss=3.69, v_num=2]Epoch 0:   0%|          | 34/10549 [01:07<5:46:27,  1.98s/it, loss=3.54, v_num=2]Epoch 0:   0%|          | 35/10549 [01:11<5:56:20,  2.03s/it, loss=3.54, v_num=2]Epoch 0:   0%|          | 35/10549 [01:11<5:56:20,  2.03s/it, loss=3.48, v_num=2]Epoch 0:   0%|          | 36/10549 [01:15<6:05:46,  2.09s/it, loss=3.48, v_num=2]Epoch 0:   0%|          | 36/10549 [01:15<6:05:46,  2.09s/it, loss=3.48, v_num=2]Epoch 0:   0%|          | 37/10549 [01:19<6:14:38,  2.14s/it, loss=3.48, v_num=2]Epoch 0:   0%|          | 37/10549 [01:19<6:14:39,  2.14s/it, loss=3.47, v_num=2]Epoch 0:   0%|          | 38/10549 [01:22<6:22:36,  2.18s/it, loss=3.47, v_num=2]Epoch 0:   0%|          | 38/10549 [01:22<6:22:36,  2.18s/it, loss=3.37, v_num=2]Epoch 0:   0%|          | 39/10549 [01:25<6:25:46,  2.20s/it, loss=3.37, v_num=2]Epoch 0:   0%|          | 39/10549 [01:25<6:25:46,  2.20s/it, loss=3.3, v_num=2] Epoch 0:   0%|          | 40/10549 [01:29<6:33:40,  2.25s/it, loss=3.3, v_num=2]Epoch 0:   0%|          | 40/10549 [01:29<6:33:40,  2.25s/it, loss=3.11, v_num=2]Epoch 0:   0%|          | 41/10549 [01:33<6:41:07,  2.29s/it, loss=3.11, v_num=2]Epoch 0:   0%|          | 41/10549 [01:33<6:41:07,  2.29s/it, loss=3.05, v_num=2]Epoch 0:   0%|          | 42/10549 [01:37<6:48:05,  2.33s/it, loss=3.05, v_num=2]Epoch 0:   0%|          | 42/10549 [01:37<6:48:05,  2.33s/it, loss=2.99, v_num=2]Epoch 0:   0%|          | 43/10549 [01:41<6:54:57,  2.37s/it, loss=2.99, v_num=2]Epoch 0:   0%|          | 43/10549 [01:41<6:54:57,  2.37s/it, loss=2.95, v_num=2]Epoch 0:   0%|          | 44/10549 [01:44<6:56:49,  2.38s/it, loss=2.95, v_num=2]Epoch 0:   0%|          | 44/10549 [01:44<6:56:49,  2.38s/it, loss=2.86, v_num=2]Epoch 0:   0%|          | 45/10549 [01:48<7:02:31,  2.41s/it, loss=2.86, v_num=2]Epoch 0:   0%|          | 45/10549 [01:48<7:02:31,  2.41s/it, loss=2.83, v_num=2]Epoch 0:   0%|          | 46/10549 [01:52<7:08:28,  2.45s/it, loss=2.83, v_num=2]Epoch 0:   0%|          | 46/10549 [01:52<7:08:28,  2.45s/it, loss=2.72, v_num=2]Epoch 0:   0%|          | 47/10549 [01:56<7:14:20,  2.48s/it, loss=2.72, v_num=2]Epoch 0:   0%|          | 47/10549 [01:56<7:14:21,  2.48s/it, loss=2.75, v_num=2]Epoch 0:   0%|          | 48/10549 [01:59<7:15:44,  2.49s/it, loss=2.75, v_num=2]Epoch 0:   0%|          | 48/10549 [01:59<7:15:44,  2.49s/it, loss=2.78, v_num=2]Epoch 0:   0%|          | 49/10549 [02:03<7:20:25,  2.52s/it, loss=2.78, v_num=2]Epoch 0:   0%|          | 49/10549 [02:03<7:20:25,  2.52s/it, loss=2.75, v_num=2]Epoch 0:   0%|          | 50/10549 [02:07<7:25:32,  2.55s/it, loss=2.75, v_num=2]Epoch 0:   0%|          | 50/10549 [02:07<7:25:32,  2.55s/it, loss=2.66, v_num=2]Epoch 0:   0%|          | 51/10549 [02:11<7:30:03,  2.57s/it, loss=2.66, v_num=2]Epoch 0:   0%|          | 51/10549 [02:11<7:30:03,  2.57s/it, loss=2.76, v_num=2]Epoch 0:   0%|          | 52/10549 [02:14<7:31:07,  2.58s/it, loss=2.76, v_num=2]Epoch 0:   0%|          | 52/10549 [02:14<7:31:08,  2.58s/it, loss=2.79, v_num=2]Epoch 0:   1%|          | 53/10549 [02:18<7:35:36,  2.60s/it, loss=2.79, v_num=2]Epoch 0:   1%|          | 53/10549 [02:18<7:35:36,  2.60s/it, loss=2.88, v_num=2]Epoch 0:   1%|          | 54/10549 [02:21<7:39:45,  2.63s/it, loss=2.88, v_num=2]Epoch 0:   1%|          | 54/10549 [02:21<7:39:45,  2.63s/it, loss=2.86, v_num=2]Epoch 0:   1%|          | 55/10549 [02:25<7:44:01,  2.65s/it, loss=2.86, v_num=2]Epoch 0:   1%|          | 55/10549 [02:25<7:44:01,  2.65s/it, loss=2.76, v_num=2]Epoch 0:   1%|          | 56/10549 [02:29<7:48:22,  2.68s/it, loss=2.76, v_num=2]Epoch 0:   1%|          | 56/10549 [02:29<7:48:23,  2.68s/it, loss=2.69, v_num=2]Epoch 0:   1%|          | 57/10549 [02:32<7:48:41,  2.68s/it, loss=2.69, v_num=2]Epoch 0:   1%|          | 57/10549 [02:32<7:48:41,  2.68s/it, loss=2.6, v_num=2] Epoch 0:   1%|          | 58/10549 [02:36<7:53:02,  2.71s/it, loss=2.6, v_num=2]Epoch 0:   1%|          | 58/10549 [02:36<7:53:02,  2.71s/it, loss=2.64, v_num=2]Epoch 0:   1%|          | 59/10549 [02:40<7:56:51,  2.73s/it, loss=2.64, v_num=2]Epoch 0:   1%|          | 59/10549 [02:40<7:56:52,  2.73s/it, loss=2.6, v_num=2] Epoch 0:   1%|          | 60/10549 [02:44<8:00:26,  2.75s/it, loss=2.6, v_num=2]Epoch 0:   1%|          | 60/10549 [02:44<8:00:26,  2.75s/it, loss=2.62, v_num=2]Epoch 0:   1%|          | 61/10549 [02:48<8:04:03,  2.77s/it, loss=2.62, v_num=2]Epoch 0:   1%|          | 61/10549 [02:48<8:04:03,  2.77s/it, loss=2.65, v_num=2]Epoch 0:   1%|          | 62/10549 [02:51<8:04:06,  2.77s/it, loss=2.65, v_num=2]Epoch 0:   1%|          | 62/10549 [02:51<8:04:07,  2.77s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 63/10549 [02:55<8:07:21,  2.79s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 63/10549 [02:55<8:07:21,  2.79s/it, loss=2.51, v_num=2]Epoch 0:   1%|          | 64/10549 [02:59<8:10:36,  2.81s/it, loss=2.51, v_num=2]Epoch 0:   1%|          | 64/10549 [02:59<8:10:36,  2.81s/it, loss=2.43, v_num=2]Epoch 0:   1%|          | 65/10549 [03:03<8:13:43,  2.83s/it, loss=2.43, v_num=2]Epoch 0:   1%|          | 65/10549 [03:03<8:13:43,  2.83s/it, loss=2.43, v_num=2]Epoch 0:   1%|          | 66/10549 [03:07<8:16:39,  2.84s/it, loss=2.43, v_num=2]Epoch 0:   1%|          | 66/10549 [03:07<8:16:39,  2.84s/it, loss=2.49, v_num=2]Epoch 0:   1%|          | 67/10549 [03:11<8:19:40,  2.86s/it, loss=2.49, v_num=2]Epoch 0:   1%|          | 67/10549 [03:11<8:19:40,  2.86s/it, loss=2.5, v_num=2] Epoch 0:   1%|          | 68/10549 [03:14<8:19:34,  2.86s/it, loss=2.5, v_num=2]Epoch 0:   1%|          | 68/10549 [03:14<8:19:34,  2.86s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 69/10549 [03:18<8:22:04,  2.87s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 69/10549 [03:18<8:22:04,  2.87s/it, loss=2.54, v_num=2]Epoch 0:   1%|          | 70/10549 [03:22<8:24:42,  2.89s/it, loss=2.54, v_num=2]Epoch 0:   1%|          | 70/10549 [03:22<8:24:42,  2.89s/it, loss=2.62, v_num=2]Epoch 0:   1%|          | 71/10549 [03:26<8:27:13,  2.90s/it, loss=2.62, v_num=2]Epoch 0:   1%|          | 71/10549 [03:26<8:27:13,  2.90s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 72/10549 [03:29<8:27:05,  2.90s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 72/10549 [03:29<8:27:05,  2.90s/it, loss=2.52, v_num=2]Epoch 0:   1%|          | 73/10549 [03:32<8:29:16,  2.92s/it, loss=2.52, v_num=2]Epoch 0:   1%|          | 73/10549 [03:32<8:29:16,  2.92s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 74/10549 [03:36<8:31:56,  2.93s/it, loss=2.53, v_num=2]Epoch 0:   1%|          | 74/10549 [03:36<8:31:56,  2.93s/it, loss=2.62, v_num=2]Epoch 0:   1%|          | 75/10549 [03:40<8:33:59,  2.94s/it, loss=2.62, v_num=2]Epoch 0:   1%|          | 75/10549 [03:40<8:33:59,  2.94s/it, loss=2.7, v_num=2] Epoch 0:   1%|          | 76/10549 [03:44<8:36:14,  2.96s/it, loss=2.7, v_num=2]Epoch 0:   1%|          | 76/10549 [03:44<8:36:14,  2.96s/it, loss=2.71, v_num=2]Epoch 0:   1%|          | 77/10549 [03:47<8:35:58,  2.96s/it, loss=2.71, v_num=2]Epoch 0:   1%|          | 77/10549 [03:47<8:35:58,  2.96s/it, loss=2.71, v_num=2]Epoch 0:   1%|          | 78/10549 [03:51<8:37:57,  2.97s/it, loss=2.71, v_num=2]Epoch 0:   1%|          | 78/10549 [03:51<8:37:57,  2.97s/it, loss=2.72, v_num=2]Epoch 0:   1%|          | 79/10549 [03:55<8:40:16,  2.98s/it, loss=2.72, v_num=2]Epoch 0:   1%|          | 79/10549 [03:55<8:40:16,  2.98s/it, loss=2.81, v_num=2]Epoch 0:   1%|          | 80/10549 [03:59<8:42:32,  2.99s/it, loss=2.81, v_num=2]Epoch 0:   1%|          | 80/10549 [03:59<8:42:33,  2.99s/it, loss=2.89, v_num=2]Epoch 0:   1%|          | 81/10549 [04:03<8:44:02,  3.00s/it, loss=2.89, v_num=2]Epoch 0:   1%|          | 81/10549 [04:03<8:44:02,  3.00s/it, loss=2.81, v_num=2]Traceback (most recent call last):
  File "/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py", line 280, in <module>
    main()
  File "/home/ubuntu/cloudfs/Fengshenbang-LM/fengshen/examples/bloomz-7b1-mt/finetune_lowsub_highinter.py", line 253, in main
    trainer.fit(model, data_model)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1283, in _run_train
    self.fit_loop.run()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 271, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 203, in advance
    batch_output = self.batch_loop.run(kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 87, in advance
    outputs = self.optimizer_loop.run(optimizers, kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py", line 200, in run
    self.advance(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 201, in advance
    result = self._run_optimization(kwargs, self._optimizers[self.optim_progress.optimizer_position])
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 248, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, kwargs.get("batch_idx", 0), closure)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 358, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1550, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/core/module.py", line 1705, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py", line 289, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 216, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/deepspeed.py", line 111, in optimizer_step
    closure_result = closure()
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 146, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 141, in closure
    self._backward_fn(step_output.closure_loss)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 304, in backward_fn
    self.trainer._call_strategy_hook("backward", loss, optimizer, opt_idx)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1704, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py", line 191, in backward
    self.precision_plugin.backward(self.lightning_module, closure_loss, optimizer, optimizer_idx, *args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/deepspeed.py", line 90, in backward
    deepspeed_engine.backward(closure_loss, *args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 11, in wrapped_fn
    return func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1799, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1953, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 51, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/ubuntu/miniconda3/envs/ghostaienv/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 1; 39.59 GiB total capacity; 34.82 GiB already allocated; 1.47 GiB free; 35.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
